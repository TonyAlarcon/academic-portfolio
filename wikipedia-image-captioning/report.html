
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Abstract &#8212; Academic Portfolio</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Apriori Algorithm" href="../apriori.html" />
    <link rel="prev" title="SVM" href="../vulnerability-prediction/SVM_LSTM.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Academic Portfolio</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../particle-accelerator/overview.html">
   Particle Accelerator Simulation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../particle-accelerator/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../particle-accelerator/cyclotron.html">
     Cyclotron Code
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../amazon-vision/project-overview.html">
   Amazon Vision Robotics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../amazon-vision/ClutterizerV2.html">
     Synthetic Image Generator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../amazon-vision/Detectron.html">
     Detectron2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../amazon-vision/Testing.html">
     Testing Accuracy
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../vulnerability-prediction/project-overview.html">
   Vulnerability Prediction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Abstract
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../apriori.html">
   Apriori Algorithm
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fwikipedia-image-captioning/report.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/wikipedia-image-captioning/report.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Abstract
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-definition">
   Problem Definition
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#related-work">
   Related Work
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methods">
   Methods
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-encoder">
     Image Encoder
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-encoder">
     Text Encoder
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#text-cleaning">
       Text Cleaning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tokenization-padding">
       Tokenization &amp; Padding
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoder">
     Decoder
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Abstract</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Abstract
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-definition">
   Problem Definition
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#related-work">
   Related Work
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methods">
   Methods
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-encoder">
     Image Encoder
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-encoder">
     Text Encoder
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#text-cleaning">
       Text Cleaning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tokenization-padding">
       Tokenization &amp; Padding
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoder">
     Decoder
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="abstract">
<h1>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">#</a></h1>
<p>This paper will attempt to introduce a deep machine learning model that can generate and retrieve the closest caption, given an image. Image captioning is inherently a cross-disciplinary task within the sub fields of Computer Vision and Natural Language Processing (NLP), which has been a popular active area of research within the Artificial Intelligence (AI) community. A successful model deals with object detection/recognition while understanding their relative spatial properties within a given scene/location. In addition, the model should produce human-readable sentences (captions) which requires both syntactic and semantic understanding of the language. To build our image-caption deep learning model, we will utilize the  Wikipedia-based Image Text (WIT) dataset presented by the Google Research as detailed in their SIGIR published work  <span id="id1">[]</span>. This rich dataset is a massive curated set of 37.5 million image-text samples across 108 languages. This dataset is ideal as it is the largest open source dataset with 11.5 million unique images over several languages.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="problem-definition">
<h1>Problem Definition<a class="headerlink" href="#problem-definition" title="Permalink to this headline">#</a></h1>
<p>The advent of the internet, social platforms and many other technological innovations have made it relatively simple to encounter large number of images in our everyday routine. Very often, these images are not paired with “alt text” and/or captions. Even some of the largest websites, i.e. Wikipedia, are susceptible to this limitation. An image-captioning model that can generate or match a description based on a given image can be utilized in automatic image indexing for content-based image retrieval tasks, and can therefore be applied to many industrial sectors including education, digital libraries, web searching, military, commerce, and bio-medicine <span id="id2">[]</span>. It is clear that open models can greatly assist and improve the accessibility and learning for all.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="related-work">
<h1>Related Work<a class="headerlink" href="#related-work" title="Permalink to this headline">#</a></h1>
<p>Several approaches have been proposed in order to solve the image to caption problem. First, one approach frames the task as an \textbf{image-sentence retrieval problem} <span id="id3">[]</span> wherein the closest caption in the image training set is retrieved and transferred over to an input test image. Alternatively, certain words in the caption may be segregated according to different features present in the training image. The caption is generated by synthesising the segregated words in the training set based on the features detected in the test image ~\cite{wu2016value}. While the aforementioned approaches have shown to achieve competitive results, the produced alt-text are limited to the words and descriptions that appear in the training set.</p>
<p>A second favored approach is to formulate the problem as a \textbf{caption generator task} that utilises a cascade of neural networks in a encoder-decoder architecture to construct an image description. Some <span id="id4">[]</span>, <span id="id5">[]</span>, <span id="id6">[]</span> utilize a \textit{inject architecture}, which employs a pretrained CNN to construct a compressed and encoded representation of an image that retains it’s essential high-level features such as faces, wheels, etc. The encoded feature vectors, along with an embedded sequence of words, are subsequently fed onto a Recurrent Neural Network (RNN), such as an LSTM, in order to predict and the next word in the sequence. This model uses the RNN as a language generator. One notable variant of the inject architecture leverages the contemporary state-of-the-art technique known as transformers. This model simply replaces the LSTM with a transformer layer.</p>
<p>In an alternative model <span id="id7">[]</span>, <span id="id8">[]</span> described as the \textit{Merge Architecture}, a pretrained CNN is similarly used to encode feature vectors. However, the CNN and LSTM network respectively operate  on the image and token sequence, independently. The outputs are concatenated in a multimodal layer that interprets both outputs and subsequently fed into the sentence generator that produces the final predicted caption. Note that the distinct difference in this architecture is that the RNN is used a language model to encode word representation.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="methods">
<h1>Methods<a class="headerlink" href="#methods" title="Permalink to this headline">#</a></h1>
<p>Our approach will utilize the Merge architecture, as shown in Figure \ref{fig:schematic}. As previously mentioned, this model utilizes a multimodal space to concatenate the image and language features. The components of this model are described in detail in the following subsections.
}</p>
<section id="image-encoder">
<h2>Image Encoder<a class="headerlink" href="#image-encoder" title="Permalink to this headline">#</a></h2>
<p>This paper utilizes the concept of transfer learning, a popular optimization method in deep learning where a model previously developed for a task is re-purposed on a second related task. It is understood that this technique not only results in faster training but can significantly improve model performance.</p>
<p>This paper utilizes the Inception-v3 architecture, a computer vision model trained on over 1 million images from the ImageNet dataset for the purpose of image analysis and object detection. The InceptionV3 model consists of two primary components, a feature extraction component and a classification component. The former is a convolutional neural network consisting of symmetric and asymmetric constituents including convolutions, average pooling, max pooling, concats, and dropouts. The latter consists of fully connected multi-layer perceptrons with a softmax layer intended for classification purposes. Image \ref{fig:Inception} depicts this architecture.</p>
<p>In order to encode our images into compressed encoded feature vectors, we first preprocess each image by resizing them to the required InceptionV3 input of 299 x 299 pixel, reshaping its dimensions and re-scaling all pixel values to the range of [-1, 1], sample-wise. The prepossessed images are subsequently fed into the InceptionV3 model which provides a feature vector of size 2048. The feature vector is further compressed to a 256 element vector using a fully connected neural network with a relu activation function, which is proceeded by a dropout layer to prevent overfitting.</p>
</section>
<section id="text-encoder">
<h2>Text Encoder<a class="headerlink" href="#text-encoder" title="Permalink to this headline">#</a></h2>
<p>The sequence processor utilizes the concepts of word embedding and Long Short-Term Memory (LSTM) neural network to encode an input text sequence into a 256 element vector. Word embedding is a technique that represents words as real-valued continuous vectors in a lower-dimensional vector space such that geometrical relationships (i.e. cosine distance) between the feature vectors accurately represent the semantic relationship between words. Namely, words with similar meanings are mapped closer in vector space. LSTMs are a special archetype of Recurrent Neural Network (RNN) that is capable of handling long term dependencies, namely, this network can remember previous observations over long sequence intervals to process data.</p>
<p>In order to produce 256 vector element encoded representation of our sequences, the following processing steps are executed on the raw text data.</p>
<section id="text-cleaning">
<h3>Text Cleaning<a class="headerlink" href="#text-cleaning" title="Permalink to this headline">#</a></h3>
<p>In order to optimize training time and performance, text cleaning is an essential step implemented in some variation for every Natural Language Processing (NLP) task. This paper preprocesses the raw alt text by applying normalization, removing unicode characters (i.e. punctuation’s, emoji’s, numbers, etc.) and removing one character words (i.e. I, a, etc..). Text normalization refers to converting all capital characters to lowercase so that words like “Hello” and “hello” are not interpreted differently by our model. Unicode  and one word characters do not add any descriptive value to our sentences, as such, they can be disregarded.</p>
</section>
<section id="tokenization-padding">
<h3>Tokenization &amp; Padding<a class="headerlink" href="#tokenization-padding" title="Permalink to this headline">#</a></h3>
<p>Deep neural networks are incapable of understanding raw text, therefore, tokenization is an essential text pre-processing technique that serves to assign an arbitrary and unique integer value to a word. Once a tokenization dictionary for a given corpus is generated, it is utilized to create vector sequences for each caption descriptions. Lastly, as all neural networks require inputs of equal length, it is important to pad each token sequence, in other words, we prepend an integer value to all token sequences such that the length of all tokenized sentences are of the same length. It is important to note that this process is distinct from word embedding as tokenization merely assigns arbitrary integer values and contain no geometrical relationships.</p>
</section>
</section>
<section id="decoder">
<h2>Decoder<a class="headerlink" href="#decoder" title="Permalink to this headline">#</a></h2>
<p>The decoder merges the 256 element feature vectors from both the image and text encoder layers via an addition operation. This is followed a hidden dense layer with 256 neuron and final output Dense layer that produces a softmax prediction over the entire output vocabulary for the next word in the sequence.</p>
<p>The prediction output is generated by a fully connected neural network, as opposed to an LSTM, only one word can be generated at one each time-step during training. More specifically, if the referenced caption contains n words, the model will produce n-1 sequences, where each sequence appends an additional word. Table \ref{training_sample} provides an example of the training sequence for a given caption.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./wikipedia-image-captioning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../vulnerability-prediction/SVM_LSTM.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">SVM</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../apriori.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Apriori Algorithm</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Tony Alarcon<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>