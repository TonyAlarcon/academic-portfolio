
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Testing Accuracy &#8212; Academic Portfolio</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Apriori Algorithm" href="../apriori.html" />
    <link rel="prev" title="Detectron2" href="Detectron.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Academic Portfolio</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks.html">
   Content with notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown-notebooks.html">
   Notebooks with MyST Markdown
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../particle-accelerator/overview.html">
   Particle Accelerator Simulation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../particle-accelerator/intro.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../particle-accelerator/cyclotron.html">
     Cyclotron Code
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../vulnerability-prediction/project-overview.html">
   Vulnerability Prediction
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="project-overview.html">
   Amazon Vision Robotics
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="ClutterizerV2.html">
     Synthetic Image Generator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Detectron.html">
     Detectron2
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Testing Accuracy
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../apriori.html">
   Apriori Algorithm
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/amazon-vision/Testing.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Famazon-vision/Testing.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/amazon-vision/Testing.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Testing Accuracy
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#importing-trained-model">
   Importing Trained Model
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#testing-validation-inference">
   Testing Validation/Inference
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-data-accuracy">
     Training Data Accuracy
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-one">
     Layout One
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-two">
     Layout Two
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-three">
     Layout Three
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-four">
     Layout Four
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-five">
     Layout Five
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Testing Accuracy</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Testing Accuracy
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#importing-trained-model">
   Importing Trained Model
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#testing-validation-inference">
   Testing Validation/Inference
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-data-accuracy">
     Training Data Accuracy
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-one">
     Layout One
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-two">
     Layout Two
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-three">
     Layout Three
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-four">
     Layout Four
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout-five">
     Layout Five
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="testing-accuracy">
<h1>Testing Accuracy<a class="headerlink" href="#testing-accuracy" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Introduction to Neural Networks (CSE 60868)</span>
<span class="c1"># University of Notre Dame</span>
<span class="c1"># Deliverable 4</span>
<span class="c1"># Name: Pedro Antonio Alarcon (palarcon)</span>
<span class="c1"># _________________________________________________________________________________________</span>
<span class="c1"># Professor: Adam Czajka</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install <span class="nv">pyyaml</span><span class="o">==</span><span class="m">5</span>.1
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">TORCH_VERSION</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">CUDA_VERSION</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;+&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;torch: &quot;</span><span class="p">,</span> <span class="n">TORCH_VERSION</span><span class="p">,</span> <span class="s2">&quot;; cuda: &quot;</span><span class="p">,</span> <span class="n">CUDA_VERSION</span><span class="p">)</span>
<span class="c1"># Install detectron2 that matches the above pytorch version</span>
<span class="c1"># See https://detectron2.readthedocs.io/tutorials/install.html for instructions</span>
<span class="o">!</span>pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/<span class="nv">$CUDA_VERSION</span>/torch<span class="nv">$TORCH_VERSION</span>/index.html
<span class="c1"># If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.</span>

<span class="c1">#exit(0)  # After installation, you may need to &quot;restart runtime&quot; in Colab. This line can also restart runtime</span>
<span class="c1"># Some basic setup:</span>
<span class="c1"># Setup detectron2 logger</span>
<span class="kn">import</span> <span class="nn">detectron2</span>
<span class="kn">from</span> <span class="nn">detectron2.utils.logger</span> <span class="kn">import</span> <span class="n">setup_logger</span>
<span class="n">setup_logger</span><span class="p">()</span>

<span class="c1"># import some common libraries</span>
<span class="o">!</span>pip install opencv-python
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">json</span><span class="o">,</span> <span class="nn">cv2</span><span class="o">,</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="c1">#from google.colab.patches import cv2_imshow</span>

<span class="c1"># import some common detectron2 utilities</span>
<span class="kn">from</span> <span class="nn">detectron2</span> <span class="kn">import</span> <span class="n">model_zoo</span>
<span class="kn">from</span> <span class="nn">detectron2.engine</span> <span class="kn">import</span> <span class="n">DefaultPredictor</span>
<span class="kn">from</span> <span class="nn">detectron2.config</span> <span class="kn">import</span> <span class="n">get_cfg</span>
<span class="kn">from</span> <span class="nn">detectron2.utils.visualizer</span> <span class="kn">import</span> <span class="n">Visualizer</span>
<span class="kn">from</span> <span class="nn">detectron2.data</span> <span class="kn">import</span> <span class="n">MetadataCatalog</span><span class="p">,</span> <span class="n">DatasetCatalog</span>
<span class="kn">from</span> <span class="nn">detectron2.data.datasets</span> <span class="kn">import</span> <span class="n">register_coco_instances</span>
<span class="kn">from</span> <span class="nn">detectron2.structures</span> <span class="kn">import</span> <span class="n">BoxMode</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.7/dist-packages (5.1)
torch:  1.10 ; cuda:  cu111
Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html
Requirement already satisfied: detectron2 in /usr/local/lib/python3.7/dist-packages (0.6+cu111)
Requirement already satisfied: Pillow&gt;=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)
Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)
Requirement already satisfied: black==21.4b2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (21.4b2)
Requirement already satisfied: fvcore&lt;0.1.6,&gt;=0.1.5 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.5.post20220414)
Requirement already satisfied: hydra-core&gt;=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.2)
Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)
Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)
Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)
Requirement already satisfied: omegaconf&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.1.2)
Requirement already satisfied: iopath&lt;0.1.10,&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.9)
Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.8.0)
Requirement already satisfied: pycocotools&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.4)
Requirement already satisfied: yacs&gt;=0.1.8 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.8)
Requirement already satisfied: termcolor&gt;=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)
Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)
Requirement already satisfied: tqdm&gt;4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.64.0)
Requirement already satisfied: mypy-extensions&gt;=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (0.4.3)
Requirement already satisfied: click&gt;=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (7.1.2)
Requirement already satisfied: typing-extensions&gt;=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (4.1.1)
Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (1.4.4)
Requirement already satisfied: regex&gt;=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (2022.3.15)
Requirement already satisfied: pathspec&lt;1,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (0.9.0)
Requirement already satisfied: typed-ast&gt;=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (1.5.3)
Requirement already satisfied: toml&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2-&gt;detectron2) (0.10.2)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore&lt;0.1.6,&gt;=0.1.5-&gt;detectron2) (5.1)
Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore&lt;0.1.6,&gt;=0.1.5-&gt;detectron2) (1.21.6)
Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core&gt;=1.1-&gt;detectron2) (4.8)
Requirement already satisfied: importlib-resources&lt;5.3 in /usr/local/lib/python3.7/dist-packages (from hydra-core&gt;=1.1-&gt;detectron2) (5.2.3)
Requirement already satisfied: zipp&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources&lt;5.3-&gt;hydra-core&gt;=1.1-&gt;detectron2) (3.8.0)
Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath&lt;0.1.10,&gt;=0.1.7-&gt;detectron2) (2.4.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;detectron2) (3.0.8)
Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;detectron2) (2.8.2)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;detectron2) (0.11.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;detectron2) (1.4.2)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;detectron2) (1.15.0)
Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (1.44.0)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (0.6.1)
Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (1.0.1)
Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (3.3.6)
Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (0.37.1)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (1.8.1)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (0.4.6)
Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (57.4.0)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (1.35.0)
Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (3.17.3)
Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (2.23.0)
Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard-&gt;detectron2) (1.0.0)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (4.8)
Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (4.2.4)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (0.2.8)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;detectron2) (1.3.1)
Requirement already satisfied: importlib-metadata&gt;=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard-&gt;detectron2) (4.11.3)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;detectron2) (0.4.8)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (3.0.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (2021.10.8)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (1.24.3)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard-&gt;detectron2) (2.10)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;detectron2) (3.2.0)
Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)
Requirement already satisfied: numpy&gt;=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#uncoment when working on Google Colab</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s2">&quot;/content/gdrive&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mounted at /content/gdrive
</pre></div>
</div>
</div>
</div>
<p>I used <a class="reference external" href="https://plainsight.ai">https://plainsight.ai</a> to perform polygon annotations and exported the data utilizing their COCO format feature. However, the images segmentations are exported in the following format “segmentation” : [[x_1, y_1], [x_2, y_2], …] which is not the format expected from detectron2. Thefore, we first need to flatten the segmentation array of arrays prior to registering with detectron. The cell below takes care of that</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Function below accepts a path to a json_file with similar COCO structure and flattents the segmentations  array</span>
<span class="sd">i.e. [[x_1, y_1], [x_2, y_2], ...] will be converted to [x_1, y_1, x_2, y_2, ...]. This is the format </span>
<span class="sd">Detectron2 is expecting</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">convert_to_COCO</span><span class="p">(</span><span class="n">json_path</span><span class="p">):</span>
  <span class="c1">#variable where we will save our new json object</span>
    <span class="n">new_json</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">json_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">json_object</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="n">annotations</span> <span class="o">=</span> <span class="n">json_object</span><span class="p">[</span><span class="s1">&#39;annotations&#39;</span><span class="p">]</span>
    <span class="c1">#function to flatten the array of arrays</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">annotations</span><span class="p">)):</span>
        <span class="n">annotations</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;segmentation&quot;</span><span class="p">]</span>  <span class="o">=</span>  <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">annotations</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;segmentation&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>

    <span class="c1">#save to json object </span>
    <span class="n">json_object</span><span class="p">[</span><span class="s1">&#39;annotations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">annotations</span>

    <span class="n">new_json</span> <span class="o">=</span> <span class="n">json_object</span>
    <span class="k">return</span> <span class="n">new_json</span>


<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">function to save a json file to selected path, note we should append the name of the file</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">save_JSON</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>


<span class="c1">#Google colab path</span>
<span class="c1">#dataset_path = &quot;/content/gdrive/MyDrive/Amazon Project/Amazon-Robotics-snapshot-001-project-coco-1646593032&quot;</span>
<span class="c1">#</span>
<span class="n">dataset_path</span> <span class="o">=</span> <span class="s1">&#39;/content/gdrive/MyDrive/Amazon Project/clutterized&#39;</span>
<span class="c1">#scratch365 path</span>
<span class="c1">#dataset_path = &quot;./Amazon-Robotics-snapshot-001-project-coco-1646593032&quot;</span>
<span class="n">old_annotations_train</span> <span class="o">=</span> <span class="n">dataset_path</span> <span class="o">+</span> <span class="s2">&quot;/train.json&quot;</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">dataset_path</span> 


<span class="n">new_json_train</span> <span class="o">=</span> <span class="n">convert_to_COCO</span><span class="p">(</span><span class="n">dataset_path</span> <span class="o">+</span> <span class="s2">&quot;/train.json&quot;</span><span class="p">)</span>
<span class="n">save_JSON</span><span class="p">(</span><span class="n">new_json_train</span><span class="p">,</span> <span class="n">dataset_path</span><span class="o">+</span><span class="s2">&quot;/new_train.json&quot;</span><span class="p">)</span>
<span class="n">annotations_train</span> <span class="o">=</span> <span class="n">dataset_path</span> <span class="o">+</span> <span class="s2">&quot;/new_train.json&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#telling detectron what to call my training dataset, path to json file, and path to images</span>
<span class="n">register_coco_instances</span><span class="p">(</span><span class="s2">&quot;my_dataset_train&quot;</span><span class="p">,</span> <span class="p">{},</span> <span class="n">annotations_train</span><span class="p">,</span> <span class="n">image_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="importing-trained-model">
<h1>Importing Trained Model<a class="headerlink" href="#importing-trained-model" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inference should use the config with parameters that are used in training</span>
<span class="c1"># cfg now already contains everything we&#39;ve set previously. We changed it a little bit for inference:</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">get_cfg</span><span class="p">()</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">merge_from_file</span><span class="p">(</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">get_config_file</span><span class="p">(</span><span class="s2">&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;</span><span class="p">))</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">DATASETS</span><span class="o">.</span><span class="n">TRAIN</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;my_dataset_train&quot;</span><span class="p">,)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">DATASETS</span><span class="o">.</span><span class="n">TEST</span> <span class="o">=</span> <span class="p">()</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">DATALOADER</span><span class="o">.</span><span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">SOLVER</span><span class="o">.</span><span class="n">IMS_PER_BATCH</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">SOLVER</span><span class="o">.</span><span class="n">BASE_LR</span> <span class="o">=</span> <span class="mf">0.00025</span>  <span class="c1"># pick a good LR</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">SOLVER</span><span class="o">.</span><span class="n">MAX_ITER</span> <span class="o">=</span> <span class="mi">600</span>  <span class="c1"># 600 iterations seems decent results</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">SOLVER</span><span class="o">.</span><span class="n">STEPS</span> <span class="o">=</span> <span class="p">[]</span>        <span class="c1"># do not decay learning rate</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">ROI_HEADS</span><span class="o">.</span><span class="n">BATCH_SIZE_PER_IMAGE</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># faster, and good enough for this toy dataset (default: 512)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">ROI_HEADS</span><span class="o">.</span><span class="n">NUM_CLASSES</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 10 classes in our customized set. (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)</span>
<span class="c1"># NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.</span>


<span class="n">cfg</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">WEIGHTS</span> <span class="o">=</span> <span class="s2">&quot;/content/gdrive/MyDrive/Amazon Project/clutterized/output/model_final.pth&quot;</span>  <span class="c1"># path to the model we just trained</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">ROI_HEADS</span><span class="o">.</span><span class="n">SCORE_THRESH_TEST</span> <span class="o">=</span> <span class="mf">0.7</span>   <span class="c1"># set a custom testing threshold</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">DefaultPredictor</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="testing-validation-inference">
<h1>Testing Validation/Inference<a class="headerlink" href="#testing-validation-inference" title="Permalink to this headline">#</a></h1>
<p>Utility Method to generate the image details from a directory of images and write it to an existing COCO JSON file:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">glob</span>
  
<span class="k">def</span> <span class="nf">add_images_to_coco</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">coco_filename</span><span class="p">):</span>
    <span class="n">image_filenames</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">recursive</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">image_filename</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">image_filenames</span><span class="p">):</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_filename</span><span class="p">)</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">size</span>
        <span class="n">image_details</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;height&quot;</span><span class="p">:</span> <span class="n">height</span><span class="p">,</span>
            <span class="s2">&quot;width&quot;</span><span class="p">:</span> <span class="n">width</span><span class="p">,</span>
            <span class="s2">&quot;file_name&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">image_filename</span><span class="p">)),</span>
        <span class="p">}</span>
        <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">image_details</span><span class="p">)</span>

    <span class="c1"># This will overwrite the image tags in the COCO JSON file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">coco_filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="n">data</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">images</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">coco_filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">coco_file</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">coco_file</span><span class="p">,</span> <span class="n">indent</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#path to test set images from top view and top light configuration</span>
<span class="c1">#google colab path</span>
<span class="n">test_dataset_path</span> <span class="o">=</span> <span class="s1">&#39;/content/gdrive/MyDrive/Amazon Project/clutterized dataset/&#39;</span>
<span class="c1">#scratch365 path </span>
<span class="c1">#test_dataset_path = &#39;./dataset_jpg/&#39;</span>

<span class="n">image_file_folders</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;layout one&#39;</span><span class="p">,</span> <span class="s1">&#39;layout two&#39;</span><span class="p">,</span> <span class="s1">&#39;layout three&#39;</span><span class="p">,</span> <span class="s1">&#39;layout four&#39;</span><span class="p">,</span> <span class="s1">&#39;layout five&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">folder</span> <span class="ow">in</span> <span class="n">image_file_folders</span><span class="p">:</span>
    <span class="n">image_directory</span> <span class="o">=</span> <span class="n">test_dataset_path</span> <span class="o">+</span> <span class="n">folder</span> <span class="o">+</span> <span class="s2">&quot;/*.jpeg&quot;</span>
    <span class="n">json_directory</span> <span class="o">=</span> <span class="n">test_dataset_path</span> <span class="o">+</span> <span class="n">folder</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">folder</span> <span class="o">+</span> <span class="s1">&#39;.json&#39;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Generating: &#39;</span><span class="p">,</span> <span class="n">json_directory</span><span class="p">)</span>
    <span class="n">add_images_to_coco</span><span class="p">(</span><span class="n">image_directory</span><span class="p">,</span> <span class="n">json_directory</span><span class="p">)</span> <span class="c1">#adds the image details, i.e. height, width, file name in the COCO Json file </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating:  /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout one/layout one.json
Generating:  /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout two/layout two.json
Generating:  /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout three/layout three.json
Generating:  /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout four/layout four.json
Generating:  /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout five/layout five.json
</pre></div>
</div>
</div>
</div>
<p>Registering all 6 different test dataset configurations. This tells detectron what to call my validation tests.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">folder</span> <span class="ow">in</span> <span class="n">image_file_folders</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">folder</span> <span class="o">+</span> <span class="s1">&#39;_val&#39;</span>
    <span class="n">json_directory</span> <span class="o">=</span> <span class="n">test_dataset_path</span> <span class="o">+</span> <span class="n">folder</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">folder</span> <span class="o">+</span> <span class="s1">&#39;.json&#39;</span>
    <span class="n">register_coco_instances</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">{},</span> <span class="n">json_directory</span><span class="p">,</span> <span class="n">test_dataset_path</span> <span class="o">+</span> <span class="n">folder</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">detectron2.utils.visualizer</span> <span class="kn">import</span> <span class="n">ColorMode</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">metadata</span><span class="p">):</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">),</span> <span class="mi">3</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span><span class="mi">65</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">objects_detected</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>  
        
        <span class="n">im</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;file_name&quot;</span><span class="p">])</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>  <span class="c1"># format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format</span>
        <span class="n">objects_detected</span> <span class="o">=</span> <span class="n">objects_detected</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;instances&quot;</span><span class="p">])</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">Visualizer</span><span class="p">(</span><span class="n">im</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span> 
                    <span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                    <span class="n">instance_mode</span><span class="o">=</span><span class="n">ColorMode</span><span class="o">.</span><span class="n">IMAGE_BW</span>   <span class="c1"># remove the colors of unsegmented pixels. This option is only available for segmentation models</span>
      <span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">draw_instance_predictions</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;instances&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">im_name</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="s2">&quot;file_name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">im_name</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> 
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="o">+</span> <span class="mi">1</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">get_image</span><span class="p">()[:,</span> <span class="p">:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">objects_detected</span>
</pre></div>
</div>
</div>
</div>
<section id="training-data-accuracy">
<h2>Training Data Accuracy<a class="headerlink" href="#training-data-accuracy" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#visualize 30 training data predictions</span>
<span class="n">train_metadata</span> <span class="o">=</span> <span class="n">MetadataCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;my_dataset_train&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DatasetCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;my_dataset_train&quot;</span><span class="p">)</span> <span class="c1">#Call the registered function and return its results (return list[dict] – dataset annotations.)</span>

<span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">45</span><span class="p">))</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="mi">17</span><span class="p">):</span>  
  

    <span class="n">im</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;file_name&quot;</span><span class="p">])</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>  <span class="c1"># format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">Visualizer</span><span class="p">(</span><span class="n">im</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">train_metadata</span><span class="p">,</span> 
                <span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                <span class="n">instance_mode</span><span class="o">=</span><span class="n">ColorMode</span><span class="o">.</span><span class="n">IMAGE_BW</span>   <span class="c1"># remove the colors of unsegmented pixels. This option is only available for segmentation models</span>
  <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">draw_instance_predictions</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;instances&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="o">+</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">get_image</span><span class="p">()[:,</span> <span class="p">:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[04/24 16:17:30 d2.data.datasets.coco]: </span>Loaded 44 images in COCO format from /content/gdrive/MyDrive/Amazon Project/clutterized/new_train.json
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;).
  max_size = (max_size + (stride - 1)) // stride * stride
/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
</pre></div>
</div>
<img alt="../_images/Testing_17_2.png" src="../_images/Testing_17_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#import the COCO Evaluator to use the COCO Metrics</span>
<span class="kn">from</span> <span class="nn">detectron2.evaluation</span> <span class="kn">import</span> <span class="n">COCOEvaluator</span><span class="p">,</span> <span class="n">inference_on_dataset</span>
<span class="kn">from</span> <span class="nn">detectron2.data</span> <span class="kn">import</span> <span class="n">build_detection_test_loader</span>

<span class="c1">#Call the COCO Evaluator function and pass the Validation Dataset</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">COCOEvaluator</span><span class="p">(</span><span class="s2">&quot;my_dataset_train&quot;</span><span class="p">,</span> <span class="n">cfg</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./output/&quot;</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">build_detection_test_loader</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;my_dataset_train&quot;</span><span class="p">)</span>

<span class="c1">#Use the created predicted model in the previous step</span>
<span class="n">inference_on_dataset</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">evaluator</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Red">WARNING</span> <span class=" -Color -Color-Green">[04/24 16:18:20 d2.evaluation.coco_evaluation]: </span>COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
<span class=" -Color -Color-Green">[04/24 16:18:21 d2.data.datasets.coco]: </span>Loaded 44 images in COCO format from /content/gdrive/MyDrive/Amazon Project/clutterized/new_train.json
<span class=" -Color -Color-Green">[04/24 16:18:21 d2.data.build]: </span>Distribution of instances among all 10 categories:
<span class=" -Color -Color-Cyan">|  category  | #instances   |  category  | #instances   |  category  | #instances   |</span>
<span class=" -Color -Color-Cyan">|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|</span>
<span class=" -Color -Color-Cyan">|    ball    | 26           | calculator | 43           | controller | 43           |</span>
<span class=" -Color -Color-Cyan">|    cup     | 38           | hair brush | 41           | hard drive | 29           |</span>
<span class=" -Color -Color-Cyan">|  head set  | 46           |  keyboard  | 50           |   mouse    | 41           |</span>
<span class=" -Color -Color-Cyan">|   tongs    | 35           |            |              |            |              |</span>
<span class=" -Color -Color-Cyan">|   total    | 392          |            |              |            |              |</span>
<span class=" -Color -Color-Green">[04/24 16:18:21 d2.data.dataset_mapper]: </span>[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style=&#39;choice&#39;)]
<span class=" -Color -Color-Green">[04/24 16:18:21 d2.data.common]: </span>Serializing 44 elements to byte tensors and concatenating them all ...
<span class=" -Color -Color-Green">[04/24 16:18:21 d2.data.common]: </span>Serialized dataset takes 2.99 MiB
<span class=" -Color -Color-Green">[04/24 16:18:21 d2.evaluation.evaluator]: </span>Start inference on 44 batches
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;).
  max_size = (max_size + (stride - 1)) // stride * stride
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[04/24 16:18:38 d2.evaluation.evaluator]: </span>Inference done 11/44. Dataloading: 0.0018 s/iter. Inference: 0.3717 s/iter. Eval: 0.9537 s/iter. Total: 1.3273 s/iter. ETA=0:00:43
<span class=" -Color -Color-Green">[04/24 16:18:43 d2.evaluation.evaluator]: </span>Inference done 15/44. Dataloading: 0.0023 s/iter. Inference: 0.3705 s/iter. Eval: 0.9605 s/iter. Total: 1.3336 s/iter. ETA=0:00:38
<span class=" -Color -Color-Green">[04/24 16:18:50 d2.evaluation.evaluator]: </span>Inference done 19/44. Dataloading: 0.0032 s/iter. Inference: 0.3726 s/iter. Eval: 1.0221 s/iter. Total: 1.3986 s/iter. ETA=0:00:34
<span class=" -Color -Color-Green">[04/24 16:18:55 d2.evaluation.evaluator]: </span>Inference done 23/44. Dataloading: 0.0042 s/iter. Inference: 0.3725 s/iter. Eval: 1.0249 s/iter. Total: 1.4026 s/iter. ETA=0:00:29
<span class=" -Color -Color-Green">[04/24 16:19:01 d2.evaluation.evaluator]: </span>Inference done 27/44. Dataloading: 0.0046 s/iter. Inference: 0.3711 s/iter. Eval: 1.0243 s/iter. Total: 1.4008 s/iter. ETA=0:00:23
<span class=" -Color -Color-Green">[04/24 16:19:06 d2.evaluation.evaluator]: </span>Inference done 31/44. Dataloading: 0.0047 s/iter. Inference: 0.3719 s/iter. Eval: 1.0113 s/iter. Total: 1.3890 s/iter. ETA=0:00:18
<span class=" -Color -Color-Green">[04/24 16:19:12 d2.evaluation.evaluator]: </span>Inference done 35/44. Dataloading: 0.0044 s/iter. Inference: 0.3724 s/iter. Eval: 1.0175 s/iter. Total: 1.3953 s/iter. ETA=0:00:12
<span class=" -Color -Color-Green">[04/24 16:19:18 d2.evaluation.evaluator]: </span>Inference done 39/44. Dataloading: 0.0042 s/iter. Inference: 0.3720 s/iter. Eval: 1.0236 s/iter. Total: 1.4007 s/iter. ETA=0:00:07
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.evaluator]: </span>Inference done 44/44. Dataloading: 0.0039 s/iter. Inference: 0.3709 s/iter. Eval: 1.0040 s/iter. Total: 1.3798 s/iter. ETA=0:00:00
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.evaluator]: </span>Total inference time: 0:00:53.894727 (1.381916 s / iter per device, on 1 devices)
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.evaluator]: </span>Total inference pure compute time: 0:00:14 (0.370860 s / iter per device, on 1 devices)
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Preparing results for COCO format ...
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Saving results to ./output/coco_instances_results.json
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>Evaluate annotation type *bbox*
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>COCOeval_opt.evaluate() finished in 0.05 seconds.
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>Accumulating evaluation results...
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>COCOeval_opt.accumulate() finished in 0.03 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.605
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.767
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.726
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.640
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.631
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.631
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.051
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.669
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 60.457 | 76.676 | 72.612 |  nan  | 5.193 | 63.984 |
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Some metrics cannot be computed and is shown as NaN.
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Per-category bbox AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| ball       | 78.839 | calculator | 59.366 | controller | 72.887 |
| cup        | 56.496 | hair brush | 53.259 | hard drive | 67.674 |
| head set   | 48.827 | keyboard   | 53.531 | mouse      | 75.284 |
| tongs      | 38.412 |            |        |            |        |
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>Evaluate annotation type *segm*
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>COCOeval_opt.evaluate() finished in 0.19 seconds.
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>Accumulating evaluation results...
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.fast_eval_api]: </span>COCOeval_opt.accumulate() finished in 0.02 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.637
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.762
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.723
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.049
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.676
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.502
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.661
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.661
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.051
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.701
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 63.743 | 76.222 | 72.330 |  nan  | 4.901 | 67.616 |
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Some metrics cannot be computed and is shown as NaN.
<span class=" -Color -Color-Green">[04/24 16:19:24 d2.evaluation.coco_evaluation]: </span>Per-category segm AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| ball       | 88.464 | calculator | 62.848 | controller | 74.922 |
| cup        | 61.654 | hair brush | 56.195 | hard drive | 69.567 |
| head set   | 49.661 | keyboard   | 51.074 | mouse      | 80.623 |
| tongs      | 42.418 |            |        |            |        |
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;bbox&#39;,
              {&#39;AP&#39;: 60.45748001928397,
               &#39;AP-ball&#39;: 78.83904461874761,
               &#39;AP-calculator&#39;: 59.36550194197745,
               &#39;AP-controller&#39;: 72.88707463183293,
               &#39;AP-cup&#39;: 56.49560581476207,
               &#39;AP-hair brush&#39;: 53.25913690270126,
               &#39;AP-hard drive&#39;: 67.6741590308099,
               &#39;AP-head set&#39;: 48.82662229374554,
               &#39;AP-keyboard&#39;: 53.53099954035015,
               &#39;AP-mouse&#39;: 75.28444057005429,
               &#39;AP-tongs&#39;: 38.412214847858415,
               &#39;AP50&#39;: 76.67605010370276,
               &#39;AP75&#39;: 72.61229462399808,
               &#39;APl&#39;: 63.984394268493475,
               &#39;APm&#39;: 5.192519251925193,
               &#39;APs&#39;: nan}),
             (&#39;segm&#39;,
              {&#39;AP&#39;: 63.74269460675998,
               &#39;AP-ball&#39;: 88.46376811594203,
               &#39;AP-calculator&#39;: 62.84841484148416,
               &#39;AP-controller&#39;: 74.92161851931799,
               &#39;AP-cup&#39;: 61.654283154067926,
               &#39;AP-hair brush&#39;: 56.19542935725934,
               &#39;AP-hard drive&#39;: 69.5668573068487,
               &#39;AP-head set&#39;: 49.661265315782174,
               &#39;AP-keyboard&#39;: 51.07407540204569,
               &#39;AP-mouse&#39;: 80.62321189549296,
               &#39;AP-tongs&#39;: 42.41802215935879,
               &#39;AP50&#39;: 76.2223420955393,
               &#39;AP75&#39;: 72.32951235498503,
               &#39;APl&#39;: 67.6156982618955,
               &#39;APm&#39;: 4.900990099009901,
               &#39;APs&#39;: nan})])
</pre></div>
</div>
</div>
</div>
</section>
<section id="layout-one">
<h2>Layout One<a class="headerlink" href="#layout-one" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#top_light_top_view</span>
<span class="n">top_light_top_view_metadata</span> <span class="o">=</span> <span class="n">MetadataCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout one_val&quot;</span><span class="p">)</span>
<span class="n">top_light_top_view_dataset</span> <span class="o">=</span> <span class="n">DatasetCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout one_val&quot;</span><span class="p">)</span>

<span class="n">count</span> <span class="o">=</span> <span class="n">visualize</span><span class="p">(</span><span class="n">top_light_top_view_dataset</span><span class="p">,</span> <span class="n">top_light_top_view_metadata</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Objects Detected in Top Light Top View Configuration: &quot;</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[04/24 16:51:53 d2.data.datasets.coco]: </span>Loaded 9 images in COCO format from /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout one/layout one.json
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;).
  max_size = (max_size + (stride - 1)) // stride * stride
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Objects Detected in Top Light Top View Configuration:  51
</pre></div>
</div>
<img alt="../_images/Testing_20_3.png" src="../_images/Testing_20_3.png" />
</div>
</div>
</section>
<section id="layout-two">
<h2>Layout Two<a class="headerlink" href="#layout-two" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#top_light_side_view</span>
<span class="n">top_light_side_view_metadata</span> <span class="o">=</span> <span class="n">MetadataCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout two_val&quot;</span><span class="p">)</span>
<span class="n">top_light_side_view_dataset</span> <span class="o">=</span> <span class="n">DatasetCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout two_val&quot;</span><span class="p">)</span>

<span class="n">count</span> <span class="o">=</span> <span class="n">visualize</span><span class="p">(</span><span class="n">top_light_side_view_dataset</span><span class="p">,</span> <span class="n">top_light_side_view_metadata</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Objects Detected in Top Light Size View Configuration: &quot;</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[04/24 16:59:38 d2.data.datasets.coco]: </span>Loaded 9 images in COCO format from /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout two/layout two.json
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;).
  max_size = (max_size + (stride - 1)) // stride * stride
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Objects Detected in Top Light Size View Configuration:  38
</pre></div>
</div>
<img alt="../_images/Testing_22_3.png" src="../_images/Testing_22_3.png" />
</div>
</div>
</section>
<section id="layout-three">
<h2>Layout Three<a class="headerlink" href="#layout-three" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#side_light_side_view</span>
<span class="n">side_light_side_view_metadata</span> <span class="o">=</span> <span class="n">MetadataCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout three_val&quot;</span><span class="p">)</span>
<span class="n">side_light_side_view_dataset</span> <span class="o">=</span> <span class="n">DatasetCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout three_val&quot;</span><span class="p">)</span>

<span class="n">count</span>  <span class="o">=</span> <span class="n">visualize</span><span class="p">(</span><span class="n">side_light_side_view_dataset</span><span class="p">,</span> <span class="n">side_light_side_view_metadata</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Objects Detected in Side Light Side View Configuration: &quot;</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[04/24 17:27:09 d2.data.datasets.coco]: </span>Loaded 9 images in COCO format from /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout three/layout three.json
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;).
  max_size = (max_size + (stride - 1)) // stride * stride
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Objects Detected in Side Light Side View Configuration:  67
</pre></div>
</div>
<img alt="../_images/Testing_24_3.png" src="../_images/Testing_24_3.png" />
</div>
</div>
</section>
<section id="layout-four">
<h2>Layout Four<a class="headerlink" href="#layout-four" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#side_light_top_view</span>
<span class="n">side_light_top_view_metadata</span> <span class="o">=</span> <span class="n">MetadataCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout four_val&quot;</span><span class="p">)</span>
<span class="n">side_light_top_view_dataset</span> <span class="o">=</span> <span class="n">DatasetCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout four_val&quot;</span><span class="p">)</span>

<span class="n">count</span>  <span class="o">=</span> <span class="n">visualize</span><span class="p">(</span><span class="n">side_light_top_view_dataset</span><span class="p">,</span> <span class="n">side_light_top_view_metadata</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Objects Detected in Side Light Top View Configuration: &quot;</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[04/24 17:04:54 d2.data.datasets.coco]: </span>Loaded 9 images in COCO format from /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout four/layout four.json
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;).
  max_size = (max_size + (stride - 1)) // stride * stride
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Objects Detected in Side Light Top View Configuration:  63
</pre></div>
</div>
<img alt="../_images/Testing_26_3.png" src="../_images/Testing_26_3.png" />
</div>
</div>
</section>
<section id="layout-five">
<h2>Layout Five<a class="headerlink" href="#layout-five" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#ambient_light_top_view</span>
<span class="n">ambient_light_top_view_metadata</span> <span class="o">=</span> <span class="n">MetadataCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout five_val&quot;</span><span class="p">)</span>
<span class="n">ambient_light_top_view_dataset</span> <span class="o">=</span> <span class="n">DatasetCatalog</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout five_val&quot;</span><span class="p">)</span>

<span class="n">count</span> <span class="o">=</span> <span class="n">visualize</span><span class="p">(</span><span class="n">ambient_light_top_view_dataset</span><span class="p">,</span> <span class="n">ambient_light_top_view_metadata</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Objects Detected in Ambient Light Top View Configuration: &quot;</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[04/24 17:05:19 d2.data.datasets.coco]: </span>Loaded 9 images in COCO format from /content/gdrive/MyDrive/Amazon Project/clutterized dataset/layout five/layout five.json
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the &#39;trunc&#39; function NOT &#39;floor&#39;). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode=&#39;trunc&#39;), or for actual floor division, use torch.div(a, b, rounding_mode=&#39;floor&#39;).
  max_size = (max_size + (stride - 1)) // stride * stride
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Objects Detected in Ambient Light Top View Configuration:  47
</pre></div>
</div>
<img alt="../_images/Testing_28_3.png" src="../_images/Testing_28_3.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./amazon-vision"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Detectron.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Detectron2</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../apriori.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Apriori Algorithm</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Tony Alarcon<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>