#!/usr/bin/env python
# coding: utf-8

# # Source Code Embeddings

# In[8]:


import pandas as pd
import h5py
from pycparser import parse_file, c_ast, c_parser
import numpy as np
from collections import Counter
from collections.abc import Iterable


# # Open and Explore Test, Train and Validation Datasets

# In[5]:


def get_dataset(path):
  df = None
  # open the file as 'f'
  with h5py.File(path, 'r') as f:
    # List all groups
    print("Keys: %s" % f.keys())
    #create a dictionary of our data
    data = dict()
    for column in list(f.keys()):
      data[column] = f[column]

    #Create Pandas Dataframe
    df = pd.DataFrame(data)

  return df


train_path = "./dataset/VDISC_train.hdf5"

df_train = get_dataset(train_path)


# In[6]:


df_train.head(5)


# # AST Generation

# Pycparser library, which
# is a parser for the C language (C99), for generating ASTs
# of the source codes

# In[9]:


text = " int main() { int a = 5, b = 2; printf(a+b); }"
parser = c_parser.CParser()
ast = parser.parse(text, filename='<none>')
ast.show(nodenames=True, )


# In[10]:


# Definition for a Node.
class Node(object):
    def __init__(self, val, children):
        self.val = val
        self.children = children


# Definition for a binary tree node.
class TreeNode(object):
    def __init__(self, x):
        self.val = x
        self.left = None
        self.right = None

class ModifiedNodeVisitor(c_ast.NodeVisitor):

  lis = list()

  def visit(self, node):
    """ Visit a node.
    """
    print(self.getNodeValue(node))

    return self.generic_visit(node)

  def generic_visit(self, node):
    """ Called if no explicit visitor function exists for a
        node. Implements preorder visiting of the node.
    """
    self.lis.append(node)
    #print("Children: ", self.getChildren(node))
    for c in node:
      print("Output of c.__class__.__name__: ", c.__class__.__name__)
      self.visit(c)

  def getNodeValue(self, node):
    attributes = []
    attributes.append(node.__class__.__name__) #this add the token name
    #this adds the remaining attribute values associated with token
    try:
      for attr in node.attr_names:
        if getattr(node, attr):
          attributes.append(getattr(node, attr))
    except:
      pass

    return attributes

  def getChildren(self, node):
    children = []
    for child in node:
      children.append(child)

    return children

#v = ModifiedNodeVisitor()
#v.generic_visit(ast)
#print(len(v.lis))


# # Transform M-Arry Tree to Binary Tree

# In[12]:


def getNodeValue(node):
  attributes = []
  attributes.append(node.__class__.__name__) #this add the token name
  #this adds the remaining attribute values associated with token
  try:
    for attr in node.attr_names:
      if getattr(node, attr):
        attributes.append(getattr(node, attr))
  except:
    pass

  return attributes

def getChildren(node):
  children = []
  for child in node:
    children.append(child)

  return children

def encode(root):
  if root == None:
    return None
  
  rootTreeNode = TreeNode(getNodeValue(root))

  children = getChildren(root)
  if children:
    rootTreeNode.right = encode(children[0])

  # the parent for the rest of the children
  currTreeNode = rootTreeNode.right

    # encode the rest of the children
  children = getChildren(root)
  for i in range(1, len(children)):
    currTreeNode.left = encode(children[i])
    currTreeNode = currTreeNode.left

  return rootTreeNode


# In[13]:


binaryTree  = encode(ast.ext[0])


# In[14]:


def test_level_order(root):
    """
    :type root: Node
    :rtype: List[List[int]]
    """
    if root == None:
        return []
    result = []
    queue = []
    queue.append(root)
    level = 0
    while len(queue) > 0:
        size = len(queue)
        nodes_on_the_same_level = []
        # iterate the nodes on the same level
        print("Size at level {}: {}".format(level, size))
        level += 1
        for i in range(size):

                # add each node to an array
            temp = queue.pop(0)
            nodes_on_the_same_level.append(temp.val)
            # add its children to the queue
            if temp.left != None:
                queue.append(temp.left)
            if temp.right != None:
                queue.append(temp.right)
        result.append(nodes_on_the_same_level)
    #print(result)
    return result

test_level_order(binaryTree)


# # Complete Binary Tree to Array Representation

# In[15]:


#pads a list with zeros if size if not 3
def pad_list(lis):
  size = len(lis)
  if size < 3:
    difference = 3 - size
    for i in range(0,difference):
      lis.append(0.0)
  return lis

  

#preprocesses a node
def preprocess_node(node):
  if node is None:
      return
  node.val = pad_list(node.val)
  if node.left is None:
    node.left = TreeNode([0.0, 0.0, 0.0])
  if node.right is None:
    node.right = TreeNode([0.0, 0.0, 0.0])
  return node

# Function to  print level order traversal of tree
def printLevelOrder(root):
  h = height(root)
  array_representation = []
  for i in range(1, h+1):
    #print("\nlevel: ", i)
    printCurrentLevel(root, i, array_representation)
    if i == 9:
      break

  return array_representation
  
# Print nodes at a current level
def printCurrentLevel(root, level, arr):
  root = preprocess_node(root)
  
  if root is None:
      return
  if level == 1:
      #print(root.val, end=" ")
      arr.append(root.val)
  elif level > 1:
      printCurrentLevel(root.left, level-1, arr)
      printCurrentLevel(root.right, level-1, arr)
  

def height(node):
    if node is None:
        return 0
    else:
        # Compute the height of each subtree
        lheight = height(node.left)
        rheight = height(node.right)
 
        # Use the larger one
        if lheight > rheight:
            return lheight+1
        else:
            return rheight+1


binary_tree = binaryTree
array_representation = printLevelOrder(binary_tree)


# In[16]:


print(array_representation)


# In[17]:


import numpy as np
from collections.abc import Iterable

def flatten(l):
    for el in l:
        if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):
            yield from flatten(el)
        else:
            yield el

#print('Original list', array_representation)
trf = list(flatten(array_representation))
print('Transformed list', trf)
print('Length of transformed list: ', len(trf))


# In[ ]:


from pycparser import c_parser
import json

def add_array_rep_column(dataframe):
  new_dataframe = dataframe #copy the dataframe to make operations on

  df_list = dataframe.values.tolist()
  results = []

  for i in range(len(dataframe)):
    if i%1000 == 0:
      print("iteration", i)
    text = new_dataframe.loc[i, 'functionSource']
    text = text.decode("utf-8")
    #print(text)
    #this needs to be included because unfortunately alot of the provided code does not contain 
    #compilable code, i.e. missing ";" after a statement or other errors. 
    try:
      parser = c_parser.CParser()
      ast_1 = parser.parse(text, filename='<none>')
      tree  = encode(ast_1.ext[0])
      arr = printLevelOrder(tree)
      arr = arr
      arrRepUnmapped = list(flatten(arr))

      temp = df_list[i] + arrRepUnmapped[:1533]
      #print(len(temp))
      results.append(temp)
    except:
      continue

  return results


# In[ ]:


df_train_array_rep = add_array_rep_column(df_train)


# # Create New Dataset

# In[ ]:


#function to insert the column names into our new dataframes
def put_columns(dataframe):
  columns = ["CWE-119","CWE-120",	"CWE-469",	"CWE-476",	"CWE-other",	"functionSource"]
  for i in range(1, 1534):
    columns.append(str(i))
  
  df = pd.DataFrame(dataframe, columns=columns)

  return df


# In[ ]:


df_train_raw = put_columns(df_train_array_rep)


# In[ ]:


df_train_raw.head(5)
#df_validate_raw.head(5)


# In[ ]:


def fillna(dataframe):
  df = dataframe
  if dataframe.iloc[:,:].isnull().values.any():
    print("datasets contain null/empty values... filling them with 0.0")
    df = df.fillna(0.0) 

  return df


# In[ ]:


df_test_raw_complete  = fillna(df_train_raw)
df_test_raw_complete.isnull().values.any()


# In[ ]:


print("Length of Compilable Raw Test Dataset: ", len(df_test_raw_complete))
#print("Length of Compilable Raw Validation Dataset: ", len(df_validation_raw_complete))


# # Creating Dataset to review "Impact of Depth" section

# In[ ]:


#path = "/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/Train/cwe_120_train.csv"
path = '/content/drive/MyDrive/Vulnerability Prediction/Raw Datasets/df_train_raw.csv'
df= pd.read_csv(path)


# In[ ]:


df.head(5)


# In[ ]:


df['CWE-120'].value_counts()


# In[ ]:


#function to filter our rows on a dataframe based on  column values
def filter_rows(dataframe, column, value):
  filter_condition = dataframe[column] == value
  result = dataframe[filter_condition]
  result = result.iloc[:, :]

  return result


# In[ ]:


def undersample_df(dataframe, column):
  cwe_true = filter_rows(dataframe, column, True)
  print(f'Length of {column} Positive Dataset: {len(cwe_true)}')

  cwe_false = filter_rows(dataframe, column, False)
  print(f'Randomly sampling {len(cwe_true)} instances from Negative Dataset')

  cwe_false_sample = cwe_false.sample(n=len(cwe_true))
  print('Concatenating Positive and Negative instances')

  complete_cwe_raw = pd.concat([cwe_true, cwe_false_sample], axis=0)
  print(f'Length of complete {column} undersampled dataset: { len(complete_cwe_raw)}\n')

  return complete_cwe_raw


# In[ ]:


cwe_119_test = undersample_df(df, 'CWE-119')
cwe_120_test = undersample_df(df, 'CWE-120')
cwe_469_test = undersample_df(df, 'CWE-469')
cwe_476_test = undersample_df(df, 'CWE-476')
cwe_other_test = undersample_df(df, 'CWE-other')


# In[ ]:


cwe_119_test.head(3)


# In[ ]:


# saving the dataframe
#cwe_119_test.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_119_test_raw.csv')
#cwe_120_test.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_120_test_raw.csv')
#cwe_469_test.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_469_test_raw.csv')
#cwe_476_test.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_476_test_raw.csv')
#cwe_other_test.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_other_test_raw.csv')


# Repeating the same procedure for the validation dataset

# In[ ]:


#cwe_119_validation = undersample_df(df_validation_raw_complete, 'CWE-119')
#cwe_120_validation = undersample_df(df_validation_raw_complete, 'CWE-120')
#cwe_469_validation = undersample_df(df_validation_raw_complete, 'CWE-469')
#cwe_476_validation = undersample_df(df_validation_raw_complete, 'CWE-476')
#cwe_other_validation = undersample_df(df_validation_raw_complete, 'CWE-other')


# In[ ]:


#cwe_119_validation.head(4)


# In[ ]:


# saving the dataframe
#cwe_119_validation.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_119_validation_raw.csv')
#cwe_120_validation.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_120_validation_raw.csv')
#cwe_469_validation.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_469_validation_raw.csv')
#cwe_476_validation.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_476_validation_raw.csv')
#cwe_other_validation.to_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/cwe_other_validation_raw.csv')


# #Encode Features

# In[ ]:


def flatten(l):
    for el in l:
        if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):
            yield from flatten(el)
        else:
            yield el

def get_words(dataframe):
  results = dataframe.values.tolist()
  results = list(flatten(results))
  return results

def filter0(variable):
  list_to_filter = [0.0, 0]

  if variable in list_to_filter:
    return False
  else:
    return True

def get_word_mapping(dataframe):
  words = get_words(dataframe) #get all values in a dataframe and flatten if it contains a list
  string_words = [str(i) for i in words] #convert it all to string 
  wordCounts = Counter(string_words) #creates a Counter
  uniqueWords = sorted(wordCounts, key=wordCounts.get, reverse=True) #removes duplicate words
  wordsToIndex = {w: i for i, w in enumerate(uniqueWords)} #gets a mapping for word to index

  return wordsToIndex


# In[ ]:


#given a feature dataframe and a word mapping, we transforms it's values to their mapping
def transform_features(feature, mapping):
  result = feature
  for col in feature:
    result[col] = result[col].astype(str) #first convert the cell values to string type
    result[col] = result[col].map(mapping) #convert string type words to their numerical mapping

  return result

def process_raw_df(dataframe):
  features = dataframe.iloc[:, 6:]
  labels = dataframe.iloc[:, :6]

  wordsToIndex = get_word_mapping(features)
  transformed_features = transform_features(features, wordsToIndex)

  result = pd.concat([labels, transformed_features], axis=1, join='inner')

  return result


# In[ ]:


def save_df(dataframe, path):
  processed = process_raw_df(dataframe)
  has_null = processed.iloc[:,:].isnull().values.any() #check if everything got converted correctly -- should print False

  print(f"Does data contain null values?: {has_null}")

  processed.to_csv(path)


# In[ ]:


cwe_119_train = pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 119/cwe_119_train_raw.csv', index_col = 0)
cwe_120_train = pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 120/cwe_120_train_raw.csv', index_col = 0)
cwe_469_train =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 469/cwe_469_train_raw.csv', index_col = 0)
cwe_476_train =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 476/cwe_476_train_raw.csv', index_col = 0)
cwe_other_train =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE Other/cwe_other_train_raw.csv', index_col = 0)

cwe_119_train.head(5)


# In[ ]:


test_path = '/content/drive/MyDrive/Vulnerability Prediction/'
save_df(cwe_119_train, test_path+'cwe_119_train.csv')
save_df(cwe_120_train, test_path+'cwe_120_train.csv')
save_df(cwe_469_train, test_path+'cwe_469_train.csv')
save_df(cwe_476_train, test_path+'cwe_476_train.csv')
save_df(cwe_other_train, test_path+'cwe_other_train.csv')


# In[ ]:




#cwe_119_validation = pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 119/cwe_119_validation_raw.csv', index_col = 0)
#cwe_120_validation = pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 120/cwe_120_validation_raw.csv', index_col = 0)
#cwe_469_validation =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 469/cwe_469_validation_raw.csv', index_col = 0)
#cwe_476_validation =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 476/cwe_476_validation_raw.csv', index_col = 0)
#cwe_other_validation =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE Other/cwe_other_validation_raw.csv', index_col = 0)

#cwe_119_validation.head(5)


# In[ ]:


#validation_path = '/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/try/validation/'

#save_df(cwe_119_validation, validation_path+'cwe_119_validation.csv')
#save_df(cwe_120_validation, validation_path+'cwe_120_validation.csv')
#save_df(cwe_469_validation, validation_path+'cwe_469_validation.csv')
#save_df(cwe_476_validation, validation_path+'cwe_476_validation.csv')
#save_df(cwe_other_validation, validation_path+'cwe_other_validation.csv')


# In[ ]:


#test_path = '/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/try/test'

#cwe_119_test = pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 119/cwe_119_test_raw.csv', index_col = 0)
#cwe_120_test = pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 120/cwe_120_test_raw.csv', index_col = 0)
#cwe_469_test =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 469/cwe_469_test_raw.csv', index_col = 0)
#cwe_476_test =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE 476/cwe_476_test_raw.csv', index_col = 0)
#cwe_other_test =  pd.read_csv('/content/drive/MyDrive/Spring 2022 Courses/Secure Software Engineering/Vulnerability Project/raw datasets/CWE Other/cwe_other_test_raw.csv', index_col = 0)

#cwe_119_test.head(5)


# In[ ]:


#save_df(cwe_119_test, test_path+'cwe_119_test.csv')
#save_df(cwe_120_test, test_path+'cwe_120_test.csv')
#save_df(cwe_469_test, test_path+'cwe_469_test.csv')
#save_df(cwe_476_test, test_path+'cwe_476_test.csv')
#save_df(cwe_other_test, test_path+'cwe_other_test.csv')


# In[ ]:




